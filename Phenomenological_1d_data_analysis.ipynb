{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phenomenological data analysis 1 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random as ran\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random as ran\n",
    "import math\n",
    "from scipy.stats import gamma\n",
    "from scipy.special import gammainc\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lattices from the simulations are stored in the ./results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_in='/results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the function to load lattices. N_lattices is the total number of lattices that we have in ./results. Remember that they have contiguous numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_lattices(N_lattices):\n",
    "    database=[]\n",
    "    for index in range(1,N_lattices+1):\n",
    "        name='latt'+str(index)\n",
    "        lattice=np.loadtxt(percorso+name+'.txt')\n",
    "        database.append(lattice)\n",
    "        if index%100==0:print(index)\n",
    "    return database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we load the lattices. The result is going to be df, a list of lists of the resulting lattices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=load_lattices(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us not define the parameters as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D=200\n",
    "b=0.2\n",
    "mu=0.01\n",
    "sigma=10\n",
    "n=b/mu\n",
    "s=10000\n",
    "ro=np.sqrt(sigma**2/b)\n",
    "lam=np.sqrt(D/mu)\n",
    "print('rho: '+str(ro))\n",
    "print('lam: '+str(lam))\n",
    "print('lam/rho: '+str(float(lam)/ro))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, we calculate the mean number of individuals in each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean=0\n",
    "for index in range(len(data)):\n",
    "        mean+=sum(list(data[index]))\n",
    "        \n",
    "mean=mean/(len(data)*(len(data[0])))\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take a radius (which in dimension one is just half of the length L) and calculate, for each lattice, the number of individuals that live in an area of length L around the center of the lattice. The outup will be a list, individuals, that at each entry has the number of individuals of one of such realizations of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "radius=80\n",
    "individuals=[]\n",
    "center_lattice=len(df[0])/2\n",
    "for lattice in df:\n",
    "    agg_sum=0\n",
    "    for latt_index in range(center_lattice-radius,center_lattice+radius):\n",
    "        agg_sum+=lattice[latt_indice]\n",
    "    individuals.append(agg_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to do a proper binning of the system. We define a maximum number of bins that we are going to consider, N_tot_bins, and a rescaling constant, rescale.<br>   \n",
    "We will individuals into batches, each of 1000 single realizations (so 1000 different number of individuals in the area), and calculate the mean number of individuals in each of the bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the batches we will also calculate the errorbars for each bin as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_tot_bins=500\n",
    "rescale=130\n",
    "valuesN=list(range(0,rescale*N_tot_bins,rescale))\n",
    "batches=[[0 for x in range(N_tot_bins)] for y in range(len(df)/1000)]\n",
    "for index in range(len(individuals)):\n",
    "    if int(individuals[index]/rescale)>Nbins:continue\n",
    "    batches[int(index/1000)][int(individuals[index]/rescale+0.5)]+=1\n",
    "\n",
    "bin_frequency=[0 for x in range(N_tot_bins)]\n",
    "for index1 in range(N_tot_bins):\n",
    "    for index2 in range(len(batches)):\n",
    "        bin_frequency[index1]+=batches[index2][index1]\n",
    "\n",
    "bin_frequency=[x/len(batches) for x in bin_frequency]\n",
    "\n",
    "stdev=[0 for x in range(N_tot_bins)]\n",
    "for index1 in range(N_tot_bins):\n",
    "    for index2 in range(len(batches)):\n",
    "        stdev[index1]+=(bin_frequency[index1]-batches[index2][index1])**2\n",
    "\n",
    "bin_frequency=[x/(1000*rescale) for x in bins]\n",
    "print(bins)\n",
    "\n",
    "stdev=[np.sqrt(x/(len(batches)))/(1000*rescale) for x in stdev]\n",
    "print(stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we open the output file. We are going to create the new folder ./analysis results, and the csv file where we are going to store the length of the segment, L, the number of individuals with the respective frequency, and the errorbar of such frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write the resulting distribution of individuals in the log file, ready to be plotted or analyzed in a second moment or with another graphic software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_out='./analysis_results/'\n",
    "with open(path_out, 'a') as csvfile:\n",
    "    fieldnames = ['L','Num_individuals','bins','errorbar']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    for index in range(len(bins)):\n",
    "        writer.writerow({'L': 2*radius,'Num_individuals':valuesN[index],'bin_frequency': bin_frequency[index],'errorbar': stdev[index]})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
